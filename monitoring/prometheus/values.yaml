# Prometheus Helm Chart Values for SightEdit Monitoring

prometheus:
  enabled: true
  
  # Prometheus server configuration
  server:
    name: prometheus-server
    image:
      repository: prom/prometheus
      tag: v2.47.0
    
    # Resource limits
    resources:
      requests:
        memory: "512Mi"
        cpu: "250m"
      limits:
        memory: "2Gi"
        cpu: "1"
    
    # Persistent storage
    persistentVolume:
      enabled: true
      size: 50Gi
      storageClass: gp3
      accessModes:
        - ReadWriteOnce
    
    # Data retention
    retention: "30d"
    retentionSize: "45GB"
    
    # Configuration
    configMapOverrides:
      prometheus.yml: |
        global:
          scrape_interval: 15s
          evaluation_interval: 15s
          external_labels:
            cluster: sightedit-prod
            environment: production
        
        # Alerting rules
        rule_files:
          - "/etc/prometheus/rules/*.yml"
        
        # Alertmanager configuration
        alerting:
          alertmanagers:
            - static_configs:
                - targets:
                  - alertmanager:9093
        
        # Scrape configurations
        scrape_configs:
          # Prometheus itself
          - job_name: 'prometheus'
            static_configs:
              - targets: ['localhost:9090']
          
          # Kubernetes API server
          - job_name: 'kubernetes-api-server'
            kubernetes_sd_configs:
              - role: endpoints
                namespaces:
                  names:
                    - default
            scheme: https
            tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              insecure_skip_verify: true
            bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
            relabel_configs:
              - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
                action: keep
                regex: default;kubernetes;https
          
          # Node Exporter
          - job_name: 'node-exporter'
            kubernetes_sd_configs:
              - role: pod
            relabel_configs:
              - source_labels: [__meta_kubernetes_pod_label_app]
                action: keep
                regex: node-exporter
          
          # SightEdit Backend
          - job_name: 'sightedit-backend'
            kubernetes_sd_configs:
              - role: endpoints
                namespaces:
                  names:
                    - sightedit
            relabel_configs:
              - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
                action: keep
                regex: true
              - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
                action: replace
                target_label: __metrics_path__
                regex: (.+)
              - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
                action: replace
                regex: ([^:]+)(?::\d+)?;(\d+)
                replacement: $1:$2
                target_label: __address__
              - action: labelmap
                regex: __meta_kubernetes_service_label_(.+)
              - source_labels: [__meta_kubernetes_namespace]
                action: replace
                target_label: kubernetes_namespace
              - source_labels: [__meta_kubernetes_service_name]
                action: replace
                target_label: kubernetes_name
          
          # cAdvisor (container metrics)
          - job_name: 'kubernetes-cadvisor'
            scheme: https
            tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              insecure_skip_verify: true
            bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
            kubernetes_sd_configs:
              - role: node
            relabel_configs:
              - action: labelmap
                regex: __meta_kubernetes_node_label_(.+)
              - target_label: __address__
                replacement: kubernetes.default.svc:443
              - source_labels: [__meta_kubernetes_node_name]
                regex: (.+)
                target_label: __metrics_path__
                replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
          
          # Kubelet metrics
          - job_name: 'kubernetes-nodes'
            scheme: https
            tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              insecure_skip_verify: true
            bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
            kubernetes_sd_configs:
              - role: node
            relabel_configs:
              - action: labelmap
                regex: __meta_kubernetes_node_label_(.+)
              - target_label: __address__
                replacement: kubernetes.default.svc:443
              - source_labels: [__meta_kubernetes_node_name]
                regex: (.+)
                target_label: __metrics_path__
                replacement: /api/v1/nodes/${1}/proxy/metrics

    # Security context
    securityContext:
      runAsUser: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      fsGroup: 65534
    
    # Service configuration
    service:
      type: ClusterIP
      port: 9090
    
    # Ingress configuration
    ingress:
      enabled: false  # Handled by main ingress

  # Node Exporter for system metrics
  nodeExporter:
    enabled: true
    image:
      repository: prom/node-exporter
      tag: v1.6.1
    
    resources:
      requests:
        memory: "64Mi"
        cpu: "50m"
      limits:
        memory: "128Mi"
        cpu: "200m"
    
    hostNetwork: true
    hostPID: true
    
    # Security context
    securityContext:
      runAsUser: 65534
      runAsGroup: 65534
      runAsNonRoot: true
    
    # Node selector to run on all nodes
    nodeSelector:
      kubernetes.io/os: linux
    
    tolerations:
      - effect: NoSchedule
        operator: Exists

  # Pushgateway for batch job metrics
  pushgateway:
    enabled: true
    image:
      repository: prom/pushgateway
      tag: v1.6.2
    
    resources:
      requests:
        memory: "64Mi"
        cpu: "50m"
      limits:
        memory: "256Mi"
        cpu: "200m"
    
    persistentVolume:
      enabled: false
    
    service:
      type: ClusterIP
      port: 9091

# Alertmanager configuration
alertmanager:
  enabled: true
  
  image:
    repository: prom/alertmanager
    tag: v0.26.0
  
  resources:
    requests:
      memory: "128Mi"
      cpu: "50m"
    limits:
      memory: "256Mi"
      cpu: "200m"
  
  persistentVolume:
    enabled: true
    size: 4Gi
    storageClass: gp3
  
  # Alertmanager configuration
  config:
    global:
      smtp_smarthost: 'localhost:587'
      smtp_from: 'alerts@sightedit.com'
      smtp_require_tls: true
    
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'web.hook'
      routes:
        - match:
            severity: critical
          receiver: 'critical-alerts'
        - match:
            severity: warning
          receiver: 'warning-alerts'
    
    receivers:
      - name: 'web.hook'
        webhook_configs:
          - url: 'http://webhook-service:8080/alerts'
            send_resolved: true
      
      - name: 'critical-alerts'
        email_configs:
          - to: 'oncall@sightedit.com'
            subject: '[CRITICAL] {{ .GroupLabels.alertname }}'
            body: |
              {{ range .Alerts }}
              Alert: {{ .Annotations.summary }}
              Description: {{ .Annotations.description }}
              Labels: {{ .Labels }}
              {{ end }}
        slack_configs:
          - api_url: 'SLACK_WEBHOOK_URL'
            channel: '#alerts-critical'
            title: '[CRITICAL] {{ .GroupLabels.alertname }}'
            text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
      
      - name: 'warning-alerts'
        email_configs:
          - to: 'team@sightedit.com'
            subject: '[WARNING] {{ .GroupLabels.alertname }}'
            body: |
              {{ range .Alerts }}
              Alert: {{ .Annotations.summary }}
              Description: {{ .Annotations.description }}
              Labels: {{ .Labels }}
              {{ end }}
        slack_configs:
          - api_url: 'SLACK_WEBHOOK_URL'
            channel: '#alerts-warning'
            title: '[WARNING] {{ .GroupLabels.alertname }}'
            text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'

# Prometheus rules
serverFiles:
  alerting_rules.yml:
    groups:
      - name: sightedit.rules
        interval: 30s
        rules:
          # High CPU usage
          - alert: HighCPUUsage
            expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: "High CPU usage detected"
              description: "CPU usage is above 80% for {{ $labels.instance }}"
          
          # High memory usage
          - alert: HighMemoryUsage
            expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 85
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: "High memory usage detected"
              description: "Memory usage is above 85% for {{ $labels.instance }}"
          
          # Application down
          - alert: SightEditBackendDown
            expr: up{job="sightedit-backend"} == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "SightEdit backend is down"
              description: "SightEdit backend service is not responding"
          
          # High error rate
          - alert: HighErrorRate
            expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) * 100 > 5
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: "High error rate detected"
              description: "Error rate is above 5% for the last 5 minutes"
          
          # High response time
          - alert: HighResponseTime
            expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: "High response time detected"
              description: "95th percentile response time is above 1 second"